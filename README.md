# Toxic-Comment-Classification-Challenge

## 1.	Problem statement

<p align="justify"> A multi-label classification problem: Construct a trained model with optimized algorithm and parameters, which can predict the type of toxicity (among six types: toxic, severe_toxic, obscene, threat, insult, identity_hate) present in a given comment. As one comment can have multiple types of toxicity at the same time it is a multi-label classification problem.</p>

## 2.	Background

<p align="justify"> Online discussions are very common these days and are indispensable in almost every domain of business, education or any other sector. However, some people take advantage of this virtual system and may harass or tease the other participants of the discussion, due to which many valuable participants may not express their thoughts at all or participate in a very limited manner. This eventually leads to the failure of such a useful and cost-effective platform for any type of discussion. In fact, because of these complications, many companies/communities completely or partially shut their online discussion forums and discard the user comments, which may be very useful for their growth and future strategy design. One solution to this problem is to assess each and every comment made by users for any type of toxicity and display only the non-toxic comments on the discussion board. Many companies such as “Conversation AI” and “Google” are already working on this problem and have provided some solutions and significantly improved the online discussion forums. However, the major issue is to allow for a specific type of toxicity to be filtered out from the comments, this option is not available in many of the available solutions. Thus, in this work, I have constructed a model which can predict different types of toxicities present in a given comment.</p>

<p align="justify"> For this task, six types of toxicities were predefined: toxic, severe_toxic, obscene, threat, insult, identity_hate. The biggest challenge in this task is that one comment can have multiple types of toxicities at the same time. Therefore, this is a multi-label classification problem where one instance can have more than one target class labels.  Moreover, no variables are pre-defined, and thus, only based on the comments text we need to identify the type of toxicity present. Hence, we need to apply the text processing tools and identify the terms which are associated with a specific type of toxicity and use them for making the prediction about the toxicity. In this work after pre-processing of the comments text specific high-frequency terms were identified and their association with particular toxicity type was discerned using random forest based variable importance method. Further, more terms were added based-on previous studies for similar tasks (https://github.com/turalus/encycloDB). Now, these terms’s frequencies in the comments were used as variables to predict the specific types of toxicities. For multi-label classification, the “mlr” and “openML” packages were primarily used. For multi-label classification two strategies are well-known, one is problem transformation and other is algorithm adaptation, but the algorithm adaptation is very limited due to unavailability of well-adapted and computationally effective algorithms. Thus, here I used the problem transformation strategy. The most optimized method, algorithms, and parameters were used for constructing the final model. This model was evaluated on the blind dataset which was 40% of kaggle training data that was not used for training but used for model evaluation. The model could achieve the mean AUC of ROC (the error metric suggested on kaggle competition web page) of 98.60 on complete training data and 89.85 on the 60% of training data. Also, predictions were made for the kaggle test dataset and the final file for upload to the kaggle competition is provided with this submission for the evaluation. </p>

## 3.	Evaluation metric or Error metric 

<p align="justify"> As it is a classification problem we can have many error metrics such as area under the curve (AUC) for receiver operating characteristic (ROC) curve, accuracy, sensitivity or true positive rate, specificity or true negative rate, false positive rate, false negative rate and so on. However, the kaggle competition suggested the mean AUC for ROC curve. This is actually the mean of AUC for ROC curve of each type of toxicity, which is kind of average performance of the classifier across all six types if toxicities.   
Here, when we talk about these parameters in the context of multi-label classification, there are two ways to calculate these parameters, one is macro-average and other is micro-average. In the case of macro-average the metric is calculated independently for each class and them averaged over all the cases, whereas in the case of micro-average the contributions from each class in terms of true positives, true negatives, false positives, and false negatives are considered to calculate the average performance across the classes. In this work, as suggested at the kaggle competition web page the macro-average method was used for calculating different types of evaluation metrics. </p>

## 4.	Data preparation 

<p align="justify"> Note: The complete r code for this section is provided as the data_preprocess.R file and figures/tables are uploaded separately. So after setting up the path you can run the complete r code but it may take some time to run (please check for r version or package availability related errors as they vary from system to system anyways the used versions are specified in the comments of r script). </p>
  
<p align="justify"> In total the training data had eight columns, first was training id, second was comment text, third was target variable named toxic where ‘0’ stands for no and ‘1’ stands for yes, fourth was target variable named severe_toxic where ‘0’ stands for no and ‘1’ stands for yes, fifth was target variable named obscene where ‘0’ stands for no and ‘1’ stands for yes, sixth was target variable named threat where ‘0’ stands for no and ‘1’ stands for yes, seventh was target variable named insult where ‘0’ stands for no and ‘1’ stands for yes, and eight was target variable named identity_hate where ‘0’ stands for no and ‘1’ stands for yes. None of the variables had any missing values, thus missing value treatment was not required. </p>

### 4.1 Splitting into train and test dataset

<p align="justify"> This step is very important to truly evaluate the trained algorithm especially in the context of bias and variance related problems. In this step, I kept some part of our data as blind set which the algorithm will never see, not even at the cross-validation stage, and only remaining part is used for training. Here I used the “random sampling without replacement” strategy to divide the complete training data from kaggle into train and test datasets. This step was required for the true evaluation of the final constructed model as the competition is over now no more ranking are assigned. I kept 60% data for training and 40% data for testing as a blind test dataset. This kind of split was successful and we had sufficient observations for the model training. Also, it is a well saying that a good algorithm should be able to learn the useful patterns (crucial for prediction) from reasonable amount (as minimum as possible) of data and 60% of the total data looked to be good enough. </p>

### 4.2 Construction of predictor variables

<p align="justify"> The only predictor variable we had in our raw data was the second variable named as “comment text”. This had a lot of special characters which were removed as they interfere with the algorithm training and does carry any value for classification. The comments variable was the text data and this kind of variable cannot be used directly for the classification purpose. Thus, I used the tools and techniques from text analytics and dummy variable encoding to derive useful variable from the comment text. There was an identifier number for each training observation named train_id. As this has no predictive power so it was just kept there for reference but not used in modelling.</p>

<p align="justify"> This variable contained a lot of special characters (including some emoji characters). So at first, all the special characters were removed using regular expression based R commands. Then text preprocessing tools were used to covert complete text into lower case and to remove punctuation marks and whitespaces. Further, the numbers were also removed as they are not associated with any type of toxicity. Then stemming and lemmatization was performed on the processed text. Further, most important words that were selected based on the term frequency of these variables in the comments. The selected variables had the lower bound for the frequency to be 250 and upper bound of frequency to be 10,000. These bound were used so as we do not select the very rare or very common words but rather select the average frequency words. With this cut-off we could obtain a total of 1,433 words, list of which is provided as the file= all_terms_1433.csv. The frequency of these words in the pre-processed comments text was used as predictor variables. Thus, at this stage we have in total 1,433 predictor variables and six dichotomous target variables. These variables are the frequency variables of specific terms which are not mentioned here due to their abusive nature. Thus, for convenience these variables were given the name from “var1” to “var1433”. </p>

<p align="justify"> After this variable encoding I have all the predictor variables in numeric format, which is great as all of the algorithms accepts numerical data and perform well with high efficiency on the numeric data. Moreover, computation with numerical data is much faster in comparison to categorical text data. Now, this numerical data need some preprocessing before I could use this for training like remove useless variables, remove multicollinearity, treat outliers, and variable selection. Details about these steps are mentioned below. </p>

### 4.3 Remove useless variables

<p align="justify"> This step is performed to remove all the variables which will have nearly no predictive power towards the prediction of our target variables and remain nearly constant throughout the training dataset. This filtering is essential because it will improve the performance of our predictive model and it will also reduce the cost of the project in terms of time and computational power. Here I removed all the variables which have the ratio of most frequent value to second most frequent value to be more than 97/3, as this high value indicates that there is a nearly constant value for that particular variable among all the observations. Also, I removed the variables which do not show variability above a particular threshold of 5% across the training set observations. After this filtration I was left with 206 predictor variables. The train_id and target variables were kept immune to such filtering. </p>

### 4.4 Remove multicollinearity

<p align="justify"> Multicollinearity occurs when one or more predictor variables are highly correlated with any other predictor variable. It means that two or more variables carry redundant information regarding the prediction of target variables. This kind of redundant variables need to removed and only one representative should be kept. This step is essential because if multicollinearity problem is not tackled well then it may lead to high bias in the modelling, and thus, predictions made by the biased model are neither accurate nor reliable. I used the variance inflation factor (VIF) parameter (results not shown as they were very similar to results from caret package functions) and the different functions (correlation based and linear combination based) of “caret” package to detect the multicollinearity in our training data. After the filtering I found that two variables “var14” and “var113” were highly correlated so “var14” was removed from the predictor variables. Further, I found that “var412” was linear combinations of other variables so it was also removed from the predictor variable. After this step, I was left with 204 predictor variables. </p>

<p align="justify"> Note: please note that this var14 or var113 names were given to specific terms which were not mentioned here as they were very abusive in nature. Also, remove useless step the variable order is disrupted. So var14 may not have the position 14 but instead 9 </p>

### 4.5 Outlier treatment

<p align="justify"> Outliers are the specific values in particular variable which has values very different from the general distribution of the variable. These are either extremely high or extremely low. These values need proper treatment otherwise they negatively impact the accuracy and predictive power of machine learning model. At first, I detected the outliers using my custom created function: this function will iterate through each predictor variable and will identify if a particular predictor variable has any outlier values. The values outside the outer fence of variable distribution were labelled as outlier values and any predictor which has even one outlier values is considered to have outliers. After detection these outlier values were treated using a custom function, this function will iterate through all the outlier values and replace them with the outer fence boundary values (upper fence outlier with upper fence boundary values and lower fence outlier with lower fence boundary values). The advantage of using this approach is that the information about very high values for specific observations is still retained at the same time now these values are down-toned so will not affect the machine learning model. This outlier treatment was performed for all the predictor variables as almost all of them had some outliers.</p>

### 4.6 Variable Importance

<p align="justify"> In this step, I evaluated the importance of each predictor variable in making accurate predictions for the target variable. As in this particular problem we have six target variables, thus, the variable importance calculation was performed separately for each target variable. This step is crucial to reduce the computational cost of the project and also to avoid resources wastage. In literature, this step has also been shown to lead an improvement in the model’s accuracy and predictive power. I have used two methods of variable importance, one is “Backward feature selection” implemented as ‘rfe’ function in caret, and other using random forest. Only results for random forest are shown here as it performed better than the other method. After performing this step for each toxicity type the top-20 variables were selected for their visual display. The line chart displaying importance of top-20 variables using random forest method for each toxicity type is pasted below. </p>

<p align="justify"> Figure 1: Line plot displaying the variable importance value for the “toxic” target variable. The overall importance values were calculated based on the mean decrease in accuracy value and the mean decrease in Gini values using random forest in “caret” package in r. </p>

![Atl Text]()
 
 
<p align="justify"> Figure 2: Line plot displaying the variable importance value for the “severe_toxic” target variable. The overall importance values were calculated based on the mean decrease in accuracy value and the mean decrease in Gini values using random forest in “caret” package in r. </p>

![Alt Text]()

 
<p align="justify"> Figure 3: Line plot displaying the variable importance value for the “obscene” target variable. The overall importance values were calculated based on the mean decrease in accuracy value and the mean decrease in Gini values using random forest in “caret” package in r. </p>

![Alt Text]()


<p align="justify"> Figure 4: Line plot displaying the variable importance value for the “threat” target variable. The overall importance values were calculated based on the mean decrease in accuracy value and the mean decrease in Gini values using random forest in “caret” package in r. </p>

![Alt Text]()


 
<p align="justify"> Figure 5: Line plot displaying the variable importance value for the “insult” target variable. The overall importance values were calculated based on the mean decrease in accuracy value and the mean decrease in Gini values using random forest in “caret” package in r. </p>

![Alt Text]()


<p align="justify"> Figure 6: Line plot displaying the variable importance value for the “identity_hate” target variable. The overall importance values were calculated based on the mean decrease in accuracy value and the mean decrease in Gini values using random forest in “caret” package in r. </p>
  
![Alt Text]()


<p align="justify"> These top-20 most important variables from each type of toxicity were combined and uniquely sorted. A total of 49 most important variables were identified by this approach. Now, these corresponding terms to these variables were identified. We found that many of the terms in these selected variables were very common words which may not seem to associate with any type of toxicity, however, these common terms/words are very useful in distinguishing among the different types of toxicities. For example “bad” is toxic but “very bad” is severely toxic, thus to distinguish among toxic and severely toxic the word very is crucial. Further, more words related to language toxicity were downloaded from a database known as “encycloDB database” (Link: https://github.com/turalus/encycloDB/blob/master/Dirty%20Words/DirtyWords.csv). These words were added to the previously selected list of 49 terms. Now in total 110 terms/words were selected for further analysis. The list of these words can be obtained in the file=final_selected_variables.csv which is also provided with this submission. Now the frequency of these words will work as the direct predictor variable for model training and subsequent predictions. Further, processing to these direct frequencies is not required as the terms and comments text both were very well pre-processed.</p>

## 5.	Multi-label Classification modelling

<p align="justify"> Note: The complete r code for this section is provided as the modeling.R file and figures/tables are uploaded separately. So after setting up the path you can run the complete r code but it may take some time to run (please check for r version or package availability related errors as they vary from system to system anyways the used versions are specified in the comments of r script). </p>
  
<p align="justify"> As we finalized the 110 most important terms/words in the previous step frequency of which will be used as the predictor variables, remember that these terms are derived from a very thorough methodology and are very efficient in predicting the different types of toxicity and also capable of making the distinction among the different types of toxicity. Thus, in this step at first the modified training data was generated from the previously defined training data. In this new data the absolute frequency of these terms is used as the predictor variables while the target variables are kept constant.  Now as our training data ready and well set for the predictive modelling I needed to perform the selection of appropriate algorithm using sample data. Then I need to tune the selected algorithm on the sampled data so that best performance of this selected algorithm can be achieved. And both of these steps need to be done with 5-fold cross validation so that I can keep an eye on the bias and variance of the algorithm at each stage. Then I trained this algorithm on the training data (60% of kaggle training data) and evaluated its performance on the blind dataset which I created in the previous step. As the algorithm has never seen this blind set data I could use its performance on this blind set to get an idea like how my algorithm will perform on the actual real world data. All these steps are discussed in detail below.</p>

### 5.1	 Algorithm Selection

<p align="justify"> As our task is a multi-label classification problem, the classification algorithm need to be used. Other complication was that each comment can have more than one type of toxicity which makes the problem as multi-label classification problem. There exists two methods for constructing the multi-label classifier, one is algorithm adaptation method and other is the problem transformation method. In the algorithm adaptation method the algorithm is modified so that it can handle the multi-label classification problem, whereas in the problem transformation method the problem is modified so that it can be handles by the usual binary or multi-class classifiers. The algorithm adaptation method is very limited as there exists very few algorithms which can perform the multi-label classification and they not very efficient in terms of computational cost and results obtained. Thus, the problem transformation methods are more popular as they give access to a vast amount of brilliant algorithms. Thus, in this work I have used the problem transformation method to construct a multi-label classifier. </p>
 
<p align="justify"> In the problem transformation method I have compared the five types of classifier wrappers:  
A.	Binary relevance: This wrapper converts the multi-label problem into the binary classification problem for each of the target variable and then constructs the independent classifier for each target variable.
B.	Classifier chains: This method is very similar to the binary relevance method but the difference is that this method allows to model the interaction or correlation between different target variables. For example, if a comment is already toxic then it is more likely for this comment to be severe toxic than a comment which is not at all toxic.
C.	Nested stacking: It is similar to classifier chains but the input data does not use the real labels instead the target labels in the input data are the estimations of the labels obtained by the already trained learners. Thus, this method also implies a hypothesis of causative hierarchical relationship among the labels.
D.	Dependent binary relevance: In this method for the training of each label the information about the other labels is also used. In the prediction process also the predictions are made in a particular order and prediction of a particular step is dependent on the predictions of previous steps. 
E.	Stacking: This method is the combination of binary relevance and the dependent binary relevance because the labels used in the training phase are derived from the binary relevance method. 

I have compared these five methods using the sample data derived using random sampling of the training data. To make the comparison of these five methods the base classifier used was the “rpart”. The comparison is mentioned in the below figures.</p>
 
<p align="justify"> Figure 7: The comparison of the five different types of problem transformation methods. Here, five standard evaluation metrics were used for the comparison, these are: accuracy (acc), f1 score (f1), hamming loss (hamloss), precision or positive predictive value (ppv), and sensitivity or true positive rate (tpr)</p>

![Alt Text]()


<p align="justify"> Figure 8: The comparison of the five different types of problem transformation methods. This figure conveys the information similar to Figure 7 but absolute values are mentioned in the matrix format. Here also, five standard evaluation metrics were used for the comparison, these are: accuracy (acc), f1 score (f1), hamming loss (hamloss), precision or positive predictive value (ppv), and sensitivity or true positive rate (tpr)</p>
  
![Alt Text]()


<p align="justify"> Please note that, in this work, as suggested at the kaggle competition web page the macro-average method was used for calculating different types of evaluation metrics.

Based on the above-mentioned results it is apparent that the classifier chains and the dependent binary relevance method performed the best. However, in terms of sensitivity the classifier chains performed better than the dependent binary relevance method. We know that we need to have a very high sensitivity for this particular problem even at the cost of losing some grip on the specificity (due to trade-off between sensitivity and specificity) because we do want to filter even a slightly toxic comment so that any person in the discussion does not get offended and stop expressing his/her thoughts. Thus, for the further steps we selected the classifier chains method of multi-label classification.

As we know that the normal binary classifiers work as the core learner for the classifier chains. Now we need to compare the different types of base classifier on our particular problem so that we can select the best available base classifier to obtain the best prediction performance. Based on the compatibility with the classifier chain method and previous knowledge I selected seven types of base classifier to make the better comparison.   These were: xgBoost, C50, cforest (conditional forest), neural network, random forest, rpart, and SVM. The performance comparison of these base classifiers is provided in the below figure. </p>
 
<p align="justify"> Figure 9: The comparison of the different types of base classifier which will work as the core learners in the final classifier chain. The accuracy is used as the metric of comparison. The box-plot was generated using the individual performance on the 5-fold cross validation. </p>
  
![Alt Text]()


<p align="justify"> This comparison involved the 5-fold cross validation. Here, the use of 5-fold cross validation was very important because this is the most efficient way I could compare these algorithms for their bias and variance. High bias of an algorithm represents that the algorithm has too many assumptions to be met while training and while making predictions which leads to poor performance. This is one of the major causes of underfitting by a particular algorithm. On the other hand, high variance shows that algorithm did not learn any pattern for the data but just memorized the training data and because of this it shows very good performance on training data but very poor performance on real data, this is a major cause of overfitting by a particular algorithm. The bias and variance can vary from algorithm to algorithm for the same training data because different algorithm has different parameters to be estimated during training and these parameters are crucial in defining the overfitting or underfitting by an algorithm. Therefore, I compared these algorithms to select the best algorithm with highest performance and very low bias and very low variance for our regression problem. From the Figure 9 it is apparent that the xgBoost outperformed all the other algorithms in terms of accuracy with low variance and low bias. Thus, this algorithm was selected for further analysis. </p>

### 5.2	 Algorithm tuning

<p align="justify"> As mentioned above xgBoost algorithm was selected for further analysis. We know that this algorithm can out-compete other algorithms because of its high accuracy, high efficiency, fast training and prediction, and high feasibility. Still, I needed to fine tune this algorithm and select the most optimal parameters so that I can achieve highest performance from this algorithm using our training dataset. This is very important because if our core learner is very accurate then the final classifier chain based on this core learner will also be very accurate. There are many parameters for this algorithm which could be tuned. Among the tree and linear version of this algorithm I used the tree version of this algorithm, this parameter is called as general parameter. Now among the booster parameters I could change many parameters like eta, gamma, max_depth, min_child_weight, max_delta_step, subsample, colsample_bytree, and nrounds. However, I chose to optimize only max_depth and nrounds parameters because they affected the performance of the algorithm at the maximum in comparison to other parameters. We know that this parameter tuning is very much problem specific and I found that varying these two parameters could get me good results so I optimized these two parameters. After trying many different values I found that max_depth value of 25 and nrounds value of 150 gave the most optimal results in terms of least variance in the accuracy value across the 5-fold cross-validation. Thus, I selected the max_depth value of 25 and nrounds value of 150 for further analysis. The box plot showing the comparison of different parameters for xgBoost is paste below. </p>
 
<p align="justify"> Figure 10: The comparison of the different xgBoost base classifier trained at different max-depth and nrounds parameters. The accuracy is used as the metric of comparison. The box-plot was generated using the individual performance on the 5-fold cross validation. </p>

![Alt Text]()


### 5.3 Training of final multi-label classifier

<p align="justify"> Now using training data (60% of training data on kaggle web page) I trained the classifier chains method based multi-label classifier, which used the xgBoost algorithm on the selected most optimal max_depth parameter value of 25 and nrounds parameter value of 150 as the core learner. The best thing about this algorithm and these optimized parameters is that it could be trained in less than five minutes, which is very crucial for the easy implementation of the predictive modelling approach used in this work. Especially, this property is very important in the cases where data is very dynamic and added in real time or every hour and now the algorithm needs to learn and update very quickly according to this new data. This final model was created using R version 3.3.2 so it can be loaded on any machine with R version 3.3.2 and can be used for making prediction on new data. As I have mentioned that this can be trained within five minutes (time could be reduced further by more optimizations) I am providing the script and data with this submission so within one minute this model can be trained for any machine with any version of R software. The final trained model is saved and provided with this submission (as .RData file) which can be loaded and used for making predictions on any new data and for any further evaluations. </p>

## 6.	Evaluation of trained model

<p align="justify"> Note: The complete r code for this section is provided as the evaluation.R file and figures/tables are uploaded separately. So after setting up the path you can run the complete r code but it may take some time to run (please check for r version or package availability related errors as they vary from system to system anyways the used versions are specified in the comments of r script). 
  
Evaluation of the trained model is essential to understand the strength and weakness of our final predicted model. Therefore I have performed two kind of testing, one is testing on the blind set and other is testing on the actual kaggle test data </p>

### 6.1 Evaluation on blind dataset

<p align="justify"> This evaluation was performed on the blind dataset which was the remaining 40% data of the kaggle training data. This blind set construction was necessary as the completion is over and no more rankings are assigned and thus, we could only use some part of training data (with known target variables) for model evaluation. This is called blind set because the algorithm has never seen this data at any stage of optimization or training. Now from these actual and predicted values for each type of toxicity I could calculate the mean performance (macro-average) across all the toxicity types and also the spate binary for each toxicity type. The mean accuracy and precision which is the macro-average across all the toxicity types were obtained to be 92.54% and 77.13%, respectively. The binary performance for each type of toxicity is mentioned in the below figures. </p>
 
<p align="justify"> Figure 11: The comparison of the performance shown separately for each toxicity type. Here also, five standard evaluation metrics were used for the comparison, these are: BAC, AUC for ROC curve, MMCE, FNR, and FPR.</p>

![Alt Text]()

 
<p align="justify"> Figure 12: The comparison of the performance shown separately for each toxicity type. Here the absolute values of the five standard evaluation metrics (BAC, AUC for ROC curve, MMCE, FNR, and FPR) are mentioned in the individual tiles.</p>

![Alt Text]()


<p align="justify"> From the individual comparison it is apparent that the predictive model is very accurate in predicting the severe toxicity, obscene, and identity hate, however, it only marginally good in predicting the threat and insult types of toxicities. Thus, more improvement in the prediction for threat and insult toxicity types is required.</p>

<p align="justify"> The mean AUC for ROC curve which is equivalent to the mean column-wise AUC of ROC curve obtained was “89.58”.  After this, the model was trained on the complete training dataset from kaggle with approximately 1000 useful terms (selected by lowering the threshold for term selection) and the mean AUC for ROC curve was obtained to be “98.60”. This is a great improvement but now the training of model takes more time (up to 1 hr), which suggest that further optimizations are required before commercial implementation of this predictive model. According to the obtained mean AUC for ROC curve value of “98.60” I would have obtained the ranking of around 1000, which is not great but given some more time I could construct a more accurate model with a very good performance and reasonable training time.</p>

### 6.2 Evaluation on kaggle test dataset

<p align="justify"> At kaggle, to obtain the leaderboard ranking, I needed to make prediction on the test data provided at kaggle. Thus, I downloaded the kaggle test data and preprocessed it so that I could create the same variables for this test dataset as were used at the time of training of algorithm. Further, the toxicities predictions were made for this kaggle test data using the constructed multi-label classification predictive model.

As per the requirement of kaggle competition rule, for each of the comment in the test data the probability of each type of toxicity was calculated. A file (kaggle_test_results_upload.xlsx) with the probability values and also the suggestive types of toxicities based-on the probability values is provided with this submission for your kind reference and evaluation. Now as the competition is over file is not uploaded on the kaggle web page.</p>

## 7.	Discussion

<p align="justify"> In this particular problem “Toxic Comment Classification Challenge” I was asked to construct a model which can make the predictions about the particular type of toxicity present the comment text provided. I made a trained multi-label classification model which could make predictions about each type of toxicity and it can also predict if there exists more than one type of toxicity in the same comment. The trained model obtained a very good performance on the blind dataset and also the prediction on the real world dataset from kaggle is provided with this submission for your kind review and evaluation. The typical features of this model are that it can make the predictions about all types of toxicities simultaneously with a very good accuracy in a very reasonable time. Also, the training and prediction time for the present model is relatively less which will be very useful for easy implementation or integration into the other software tools.

Still there are some improvements which could be done, and I could not implement them due to some time and resource limitations. Firstly, while selecting the useful terms I could select only top-20 for each type of toxicity for my further analysis because if I increase more, my system cannot handle such large data. This is where I felt the need of cloud computing where we can get a large RAM and huge processing power to analyse and model such large data. Given with this facility I could add these more important variables and this will essentially improve the model performance. Also due to the limited availability of the algorithm adaptation method (now available only for random forest) this method could not be included in this work.
Additionally, I performed the complete coding in R which is sometimes very slow and not very memory efficient. Moreover, some algorithms like neural networks can be much customized to obtain good performance like by changing the number of hidden layers and trying out different activation functions and so on, but all this is not possible in R. For that I am now working on to perform this complete project in python so that I can have access to all these parameters of this powerful algorithm and also trying to use some of the more advance tools of deep learning to obtain better performance. Soon, I will also upload this python project on the Gihub repository.
Another advantage of using python for this task is that deployment (online or integration to existing software) becomes very easy because python has very strong API versions which can be very easily exploited for deployment purposes. Also, python has tools like”flask” which can be used for the web-based deployment this compete model which can be directly consumed by the end users.

## 8.	Conclusion

In this project, a machine learning model has been constructed which can predict the different type of toxicities present in a particular comment text. The comment data was very noisy and had a lot of unstructured text data, so the data filtering and preprocessing was performed and also some text analytics tools were utilized for this purpose. The most important terms/words for making the predictions about specific types of toxicities were identified and their absolute frequencies were used as the predictor variables for training the machine learning model. Since one comment could have more than one type of toxicity, the multi-label classification was performed. Further, different methods and core algorithms were compared using their performance, bias, and variance on the training data to select the most appropriate algorithm for this task. Now the selected algorithm is further tuned for its most optimized parameters to obtain the best possible performance. The final model was trained using optimized parameters and its performance was evaluated on the blind dataset and the kaggle test dataset. The model performed really well on the blind set with the macro-average accuracy of 92.54%. All the supporting files and r scripts are attached with this submission for the kind reference and evaluation. Some code is commented out as that was run on the cluster but its output is provided and loaded directly so that the rest of the script can run smoothly. 

## 9. Declaration
I certify that this complete project was performed solely by me and the report presented here is novel and free from plagiarism.</p>

Thanks,

Yours sincerely,

Shubham Kumar Jaiswal

Registered email: shubhamj.jaiswal89@gmail.com
